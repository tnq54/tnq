import os
import threading
import telebot
import numpy as np
from huggingface_hub import InferenceClient
from flask import Flask

# --- C·∫§U H√åNH ---
BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
HF_TOKEN = os.getenv("HF_TOKEN")

# --- KH·ªûI T·∫†O ---
bot = telebot.TeleBot(BOT_TOKEN)
app = Flask(__name__)
client = InferenceClient(api_key=HF_TOKEN)

# --- MODELS ---
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CHAT_MODEL = "HuggingFaceH4/zephyr-7b-beta"

# --- üìö KHO TRI TH·ª®C (RAG) ---
# ƒê√¢y l√† "n√£o ph·ª•" c·ªßa Bot. S·∫øp s·ª≠a n·ªôi dung trong n√†y nh√©.
KNOWLEDGE_BASE = [
    "T√¥i l√† Tr·ª£ l√Ω AI TNQ, ch·∫°y tr√™n n·ªÅn t·∫£ng Render v·ªõi framework Flask.",
    "T√¥i s·ª≠ d·ª•ng model 'Zephyr 7B Beta' t·ª´ Hugging Face.",
    "S·∫øp ƒëang d·∫°y t√¥i k·ªπ thu·∫≠t RAG ƒë·ªÉ t√¥i tr·∫£ l·ªùi th√¥ng minh h∆°n.",
    "M√£ ngu·ªìn c·ªßa t√¥i k·∫øt h·ª£p gi·ªØa Flask (ƒë·ªÉ gi·ªØ server s·ªëng) v√† Telebot (ƒë·ªÉ chat).",
    "S·ªü th√≠ch c·ªßa s·∫øp l√† l·∫≠p tr√¨nh AI v√† t·ªëi ∆∞u h√≥a h·ªá th·ªëng t·ª± ƒë·ªông."
]

# Bi·∫øn l∆∞u tr·ªØ Vector (B·ªô nh·ªõ t·∫°m)
VECTOR_DB = []

def get_embedding(text):
    """G·ªçi Hugging Face API ƒë·ªÉ l·∫•y embedding"""
    try:
        # feature_extraction tr·∫£ v·ªÅ ndarray
        output = client.feature_extraction(text, model=EMBEDDING_MODEL)

        # X·ª≠ l√Ω output shape (th∆∞·ªùng l√† (1, 384))
        if isinstance(output, np.ndarray) and output.ndim == 2:
            return output[0]
        if isinstance(output, list) and len(output) > 0 and isinstance(output[0], list):
            return np.array(output[0])

        return np.array(output)
    except Exception as e:
        print(f"L·ªói embedding: {e}")
        return None

def build_vector_db():
    """M√£ h√≥a vƒÉn b·∫£n th√†nh s·ªë (Vector) ƒë·ªÉ t√¨m ki·∫øm"""
    global VECTOR_DB
    print("--- ƒêANG N·∫†P D·ªÆ LI·ªÜU RAG... ---")
    try:
        for text in KNOWLEDGE_BASE:
            embed = get_embedding(text)
            if embed is not None:
                VECTOR_DB.append({"text": text, "embedding": embed})
        print(f"--- ƒê√É N·∫†P XONG {len(VECTOR_DB)} M·∫¢NH KI·∫æN TH·ª®C ---")
    except Exception as e:
        print(f"L·ªñI BUILD DB: {e}")

def find_best_context(query_text):
    """T√¨m th√¥ng tin li√™n quan nh·∫•t trong kho n√£o"""
    if not VECTOR_DB: return None
    try:
        query_embed = get_embedding(query_text)
        if query_embed is None: return None
        
        # So s√°nh v·ªõi kho d·ªØ li·ªáu
        best_score = -1
        best_text = ""
        for item in VECTOR_DB:
            # T√≠nh cosine similarity (gi·∫£ s·ª≠ vector ƒë√£ chu·∫©n h√≥a ho·∫∑c d√πng dot product ƒë∆°n gi·∫£n)
            # Embedding t·ª´ HF th∆∞·ªùng l√† 1D array cho 1 c√¢u input
            score = np.dot(query_embed, item["embedding"])
            if score > best_score:
                best_score = score
                best_text = item["text"]
        
        # Ch·ªâ l·∫•y n·∫øu ƒë·ªô gi·ªëng > 0.4 (threshold t√πy ch·ªânh)
        return best_text if best_score > 0.4 else None
    except Exception as e:
        print(f"L·ªói t√¨m ki·∫øm: {e}")
        return None

# --- WEB SERVER (ƒê·ªÉ Render kh√¥ng t·∫Øt Bot) ---
@app.route('/')
def home():
    return "Bot TNQ ƒëang ch·∫°y RAG v·ªõi Hugging Face!"

def run_web_server():
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)

# --- X·ª¨ L√ù TIN NH·∫ÆN ---
@bot.message_handler(func=lambda message: True)
def handle_message(message):
    chat_id = message.chat.id
    user_text = message.text
    bot.send_chat_action(chat_id, 'typing')

    try:
        # B∆Ø·ªöC 1: T√¨m ki·∫øm th√¥ng tin (RAG)
        context_info = find_best_context(user_text)
        
        # B∆Ø·ªöC 2: T·∫°o Prompt
        messages = []
        if context_info:
            sys_instruct = f"""
            B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch. D∆∞·ªõi ƒë√¢y l√† th√¥ng tin b·ªëi c·∫£nh li√™n quan:
            "{context_info}"
            H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n th√¥ng tin n√†y.
            N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y d√πng ki·∫øn th·ª©c c·ªßa b·∫°n nh∆∞ng ∆∞u ti√™n b·ªëi c·∫£nh tr√™n.
            Tr·∫£ l·ªùi b·∫±ng ng√¥n ng·ªØ c·ªßa ng∆∞·ªùi d√πng (Ti·∫øng Vi·ªát/Anh).
            """
        else:
            sys_instruct = "B·∫°n l√† tr·ª£ l√Ω AI h·ªØu √≠ch. H√£y tr·∫£ l·ªùi th√¢n thi·ªán v√† ng·∫Øn g·ªçn."

        messages.append({"role": "system", "content": sys_instruct})
        messages.append({"role": "user", "content": user_text})

        # B∆Ø·ªöC 3: G·ªçi Hugging Face Inference API
        response = client.chat_completion(
            model=CHAT_MODEL,
            messages=messages,
            max_tokens=500
        )
        
        bot_reply = response.choices[0].message.content
        bot.reply_to(message, bot_reply, parse_mode='Markdown')

    except Exception as e:
        print(f"L·ªói: {e}")
        bot.reply_to(message, f"L·ªói x·ª≠ l√Ω: {str(e)}")

# --- CH·∫†Y CH∆Ø∆†NG TR√åNH ---
if __name__ == "__main__":
    # 1. N·∫°p ki·∫øn th·ª©c v√†o n√£o tr∆∞·ªõc
    build_vector_db()
    
    # 2. Ch·∫°y Web Server lu·ªìng ph·ª•
    t = threading.Thread(target=run_web_server)
    t.start()
    
    # 3. Ch·∫°y Bot lu·ªìng ch√≠nh
    print("Bot ƒëang kh·ªüi ƒë·ªông v·ªõi Hugging Face...")
    try:
        bot.infinity_polling()
    except Exception as e:
        print(f"Bot b·ªã s·∫≠p: {e}")
